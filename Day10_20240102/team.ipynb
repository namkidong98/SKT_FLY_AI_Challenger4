{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_df = pd.read_csv('gam.csv')\n",
    "gyul_df = pd.read_csv('gyul.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    df = df[df['item'] == 'TG']\n",
    "    df = df[df['corporation'] == 'A']\n",
    "    df = df[df['location'] == 'J']\n",
    "    df = df[df['price(원/kg)'] > 0]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    return df\n",
    "\n",
    "df = get_data(gam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = df[['month', 'day', 'supply(kg)']]\n",
    "label_df = df[['price(원/kg)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "input_scaled = scaler.fit_transform(input_df)\n",
    "input_scaled = torch.Tensor(input_scaled).float()\n",
    "label_scaling = 1000\n",
    "target = torch.Tensor(label_df.to_numpy()).float() / label_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 100\n",
    "x_train = input_scaled[:-num_test,:]\n",
    "y_train = target[:-num_test]\n",
    "x_test = input_scaled[-num_test:,:]\n",
    "y_test = target[-num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "input_size = 3\n",
    "hidden_size = 128\n",
    "batch_size = 16\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "train_dataset = PriceDataset(x_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = PriceDataset(x_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.005\n",
    "model = CustomModel(input_size, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def train_model(model, train_loader, epochs):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (inputs, prices) in enumerate(train_loader):\n",
    "            outputs = model(inputs.cuda())\n",
    "            loss = criterion(outputs, prices.cuda())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "        \n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # 테스트 과정\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(test_dataset) * label_scaling\n",
    "    print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 9.3039\n",
      "Epoch [2/300], Loss: 8.5726\n",
      "Epoch [3/300], Loss: 8.6175\n",
      "Epoch [4/300], Loss: 8.2966\n",
      "Epoch [5/300], Loss: 8.3447\n",
      "Epoch [6/300], Loss: 8.2431\n",
      "Epoch [7/300], Loss: 8.1876\n",
      "Epoch [8/300], Loss: 7.9480\n",
      "Epoch [9/300], Loss: 8.1659\n",
      "Epoch [10/300], Loss: 7.8819\n",
      "Epoch [11/300], Loss: 7.7638\n",
      "Epoch [12/300], Loss: 7.5778\n",
      "Epoch [13/300], Loss: 7.1581\n",
      "Epoch [14/300], Loss: 6.9147\n",
      "Epoch [15/300], Loss: 6.7362\n",
      "Epoch [16/300], Loss: 6.4904\n",
      "Epoch [17/300], Loss: 5.9126\n",
      "Epoch [18/300], Loss: 5.6860\n",
      "Epoch [19/300], Loss: 5.7985\n",
      "Epoch [20/300], Loss: 5.2308\n",
      "Epoch [21/300], Loss: 5.2502\n",
      "Epoch [22/300], Loss: 5.2788\n",
      "Epoch [23/300], Loss: 4.7330\n",
      "Epoch [24/300], Loss: 4.6696\n",
      "Epoch [25/300], Loss: 4.5301\n",
      "Epoch [26/300], Loss: 4.4160\n",
      "Epoch [27/300], Loss: 4.9512\n",
      "Epoch [28/300], Loss: 4.6233\n",
      "Epoch [29/300], Loss: 4.6194\n",
      "Epoch [30/300], Loss: 5.2313\n",
      "Epoch [31/300], Loss: 4.8027\n",
      "Epoch [32/300], Loss: 4.0618\n",
      "Epoch [33/300], Loss: 3.9817\n",
      "Epoch [34/300], Loss: 3.7759\n",
      "Epoch [35/300], Loss: 3.5888\n",
      "Epoch [36/300], Loss: 3.6812\n",
      "Epoch [37/300], Loss: 4.1665\n",
      "Epoch [38/300], Loss: 3.7566\n",
      "Epoch [39/300], Loss: 3.5619\n",
      "Epoch [40/300], Loss: 4.0382\n",
      "Epoch [41/300], Loss: 3.6557\n",
      "Epoch [42/300], Loss: 4.1485\n",
      "Epoch [43/300], Loss: 3.2405\n",
      "Epoch [44/300], Loss: 3.3873\n",
      "Epoch [45/300], Loss: 3.7328\n",
      "Epoch [46/300], Loss: 3.5876\n",
      "Epoch [47/300], Loss: 3.1642\n",
      "Epoch [48/300], Loss: 3.1465\n",
      "Epoch [49/300], Loss: 3.6810\n",
      "Epoch [50/300], Loss: 3.4149\n",
      "Epoch [51/300], Loss: 3.3821\n",
      "Epoch [52/300], Loss: 3.2393\n",
      "Epoch [53/300], Loss: 3.2905\n",
      "Epoch [54/300], Loss: 3.4340\n",
      "Epoch [55/300], Loss: 3.2997\n",
      "Epoch [56/300], Loss: 3.1304\n",
      "Epoch [57/300], Loss: 3.5591\n",
      "Epoch [58/300], Loss: 3.6791\n",
      "Epoch [59/300], Loss: 3.1805\n",
      "Epoch [60/300], Loss: 3.0563\n",
      "Epoch [61/300], Loss: 3.3734\n",
      "Epoch [62/300], Loss: 3.2071\n",
      "Epoch [63/300], Loss: 3.0804\n",
      "Epoch [64/300], Loss: 3.2346\n",
      "Epoch [65/300], Loss: 3.1308\n",
      "Epoch [66/300], Loss: 3.0279\n",
      "Epoch [67/300], Loss: 3.0400\n",
      "Epoch [68/300], Loss: 2.9443\n",
      "Epoch [69/300], Loss: 3.0853\n",
      "Epoch [70/300], Loss: 3.3828\n",
      "Epoch [71/300], Loss: 3.2440\n",
      "Epoch [72/300], Loss: 3.1692\n",
      "Epoch [73/300], Loss: 3.1039\n",
      "Epoch [74/300], Loss: 3.7007\n",
      "Epoch [75/300], Loss: 3.1344\n",
      "Epoch [76/300], Loss: 3.8664\n",
      "Epoch [77/300], Loss: 2.9696\n",
      "Epoch [78/300], Loss: 3.2610\n",
      "Epoch [79/300], Loss: 3.1711\n",
      "Epoch [80/300], Loss: 3.0475\n",
      "Epoch [81/300], Loss: 3.2433\n",
      "Epoch [82/300], Loss: 3.0381\n",
      "Epoch [83/300], Loss: 3.0117\n",
      "Epoch [84/300], Loss: 2.9166\n",
      "Epoch [85/300], Loss: 2.8664\n",
      "Epoch [86/300], Loss: 2.8653\n",
      "Epoch [87/300], Loss: 3.3805\n",
      "Epoch [88/300], Loss: 3.0837\n",
      "Epoch [89/300], Loss: 2.9037\n",
      "Epoch [90/300], Loss: 2.9481\n",
      "Epoch [91/300], Loss: 2.9037\n",
      "Epoch [92/300], Loss: 2.7321\n",
      "Epoch [93/300], Loss: 3.0298\n",
      "Epoch [94/300], Loss: 3.0787\n",
      "Epoch [95/300], Loss: 2.8222\n",
      "Epoch [96/300], Loss: 2.8736\n",
      "Epoch [97/300], Loss: 2.8622\n",
      "Epoch [98/300], Loss: 2.8143\n",
      "Epoch [99/300], Loss: 3.1131\n",
      "Epoch [100/300], Loss: 3.0023\n",
      "Epoch [101/300], Loss: 2.9148\n",
      "Epoch [102/300], Loss: 3.3860\n",
      "Epoch [103/300], Loss: 2.7381\n",
      "Epoch [104/300], Loss: 2.9873\n",
      "Epoch [105/300], Loss: 2.7221\n",
      "Epoch [106/300], Loss: 2.6972\n",
      "Epoch [107/300], Loss: 2.8255\n",
      "Epoch [108/300], Loss: 3.2423\n",
      "Epoch [109/300], Loss: 2.9935\n",
      "Epoch [110/300], Loss: 2.9033\n",
      "Epoch [111/300], Loss: 2.8472\n",
      "Epoch [112/300], Loss: 2.6499\n",
      "Epoch [113/300], Loss: 2.8107\n",
      "Epoch [114/300], Loss: 2.9667\n",
      "Epoch [115/300], Loss: 2.7404\n",
      "Epoch [116/300], Loss: 3.2031\n",
      "Epoch [117/300], Loss: 2.7676\n",
      "Epoch [118/300], Loss: 3.4567\n",
      "Epoch [119/300], Loss: 2.7839\n",
      "Epoch [120/300], Loss: 2.8458\n",
      "Epoch [121/300], Loss: 2.7830\n",
      "Epoch [122/300], Loss: 2.8043\n",
      "Epoch [123/300], Loss: 2.9706\n",
      "Epoch [124/300], Loss: 2.7555\n",
      "Epoch [125/300], Loss: 2.8071\n",
      "Epoch [126/300], Loss: 3.0018\n",
      "Epoch [127/300], Loss: 2.7904\n",
      "Epoch [128/300], Loss: 2.8233\n",
      "Epoch [129/300], Loss: 2.8024\n",
      "Epoch [130/300], Loss: 2.7258\n",
      "Epoch [131/300], Loss: 2.6613\n",
      "Epoch [132/300], Loss: 2.9759\n",
      "Epoch [133/300], Loss: 2.7373\n",
      "Epoch [134/300], Loss: 2.7145\n",
      "Epoch [135/300], Loss: 2.5783\n",
      "Epoch [136/300], Loss: 2.9219\n",
      "Epoch [137/300], Loss: 3.1745\n",
      "Epoch [138/300], Loss: 3.0153\n",
      "Epoch [139/300], Loss: 2.8303\n",
      "Epoch [140/300], Loss: 3.1750\n",
      "Epoch [141/300], Loss: 2.8830\n",
      "Epoch [142/300], Loss: 3.1525\n",
      "Epoch [143/300], Loss: 2.7462\n",
      "Epoch [144/300], Loss: 2.8997\n",
      "Epoch [145/300], Loss: 2.9461\n",
      "Epoch [146/300], Loss: 2.7047\n",
      "Epoch [147/300], Loss: 2.6056\n",
      "Epoch [148/300], Loss: 2.7465\n",
      "Epoch [149/300], Loss: 2.9978\n",
      "Epoch [150/300], Loss: 2.7942\n",
      "Epoch [151/300], Loss: 2.7421\n",
      "Epoch [152/300], Loss: 2.7600\n",
      "Epoch [153/300], Loss: 2.7565\n",
      "Epoch [154/300], Loss: 2.6338\n",
      "Epoch [155/300], Loss: 2.7012\n",
      "Epoch [156/300], Loss: 2.9125\n",
      "Epoch [157/300], Loss: 2.7301\n",
      "Epoch [158/300], Loss: 2.7729\n",
      "Epoch [159/300], Loss: 2.5816\n",
      "Epoch [160/300], Loss: 2.7807\n",
      "Epoch [161/300], Loss: 2.8542\n",
      "Epoch [162/300], Loss: 3.3834\n",
      "Epoch [163/300], Loss: 3.3262\n",
      "Epoch [164/300], Loss: 3.0050\n",
      "Epoch [165/300], Loss: 2.6906\n",
      "Epoch [166/300], Loss: 2.6981\n",
      "Epoch [167/300], Loss: 2.9631\n",
      "Epoch [168/300], Loss: 2.8160\n",
      "Epoch [169/300], Loss: 2.7604\n",
      "Epoch [170/300], Loss: 2.9194\n",
      "Epoch [171/300], Loss: 2.7641\n",
      "Epoch [172/300], Loss: 2.8213\n",
      "Epoch [173/300], Loss: 2.7713\n",
      "Epoch [174/300], Loss: 2.7223\n",
      "Epoch [175/300], Loss: 3.0967\n",
      "Epoch [176/300], Loss: 2.5469\n",
      "Epoch [177/300], Loss: 2.6048\n",
      "Epoch [178/300], Loss: 2.8005\n",
      "Epoch [179/300], Loss: 2.6883\n",
      "Epoch [180/300], Loss: 2.5448\n",
      "Epoch [181/300], Loss: 3.0595\n",
      "Epoch [182/300], Loss: 2.5582\n",
      "Epoch [183/300], Loss: 2.6183\n",
      "Epoch [184/300], Loss: 2.9291\n",
      "Epoch [185/300], Loss: 2.7458\n",
      "Epoch [186/300], Loss: 2.5840\n",
      "Epoch [187/300], Loss: 2.7622\n",
      "Epoch [188/300], Loss: 2.8098\n",
      "Epoch [189/300], Loss: 2.6873\n",
      "Epoch [190/300], Loss: 2.6660\n",
      "Epoch [191/300], Loss: 2.7979\n",
      "Epoch [192/300], Loss: 2.5352\n",
      "Epoch [193/300], Loss: 2.9995\n",
      "Epoch [194/300], Loss: 2.9850\n",
      "Epoch [195/300], Loss: 2.6453\n",
      "Epoch [196/300], Loss: 2.6414\n",
      "Epoch [197/300], Loss: 2.9352\n",
      "Epoch [198/300], Loss: 2.7765\n",
      "Epoch [199/300], Loss: 2.8151\n",
      "Epoch [200/300], Loss: 2.6822\n",
      "Epoch [201/300], Loss: 2.4788\n",
      "Epoch [202/300], Loss: 2.6087\n",
      "Epoch [203/300], Loss: 2.6254\n",
      "Epoch [204/300], Loss: 2.7096\n",
      "Epoch [205/300], Loss: 2.7345\n",
      "Epoch [206/300], Loss: 2.6869\n",
      "Epoch [207/300], Loss: 2.5377\n",
      "Epoch [208/300], Loss: 2.6686\n",
      "Epoch [209/300], Loss: 2.8082\n",
      "Epoch [210/300], Loss: 2.6667\n",
      "Epoch [211/300], Loss: 2.5607\n",
      "Epoch [212/300], Loss: 2.5415\n",
      "Epoch [213/300], Loss: 2.6742\n",
      "Epoch [214/300], Loss: 2.9213\n",
      "Epoch [215/300], Loss: 2.7758\n",
      "Epoch [216/300], Loss: 2.6492\n",
      "Epoch [217/300], Loss: 2.8378\n",
      "Epoch [218/300], Loss: 2.6615\n",
      "Epoch [219/300], Loss: 2.9055\n",
      "Epoch [220/300], Loss: 2.8808\n",
      "Epoch [221/300], Loss: 2.6953\n",
      "Epoch [222/300], Loss: 2.8118\n",
      "Epoch [223/300], Loss: 2.5609\n",
      "Epoch [224/300], Loss: 2.8764\n",
      "Epoch [225/300], Loss: 2.5566\n",
      "Epoch [226/300], Loss: 2.9082\n",
      "Epoch [227/300], Loss: 2.5436\n",
      "Epoch [228/300], Loss: 2.6014\n",
      "Epoch [229/300], Loss: 2.7050\n",
      "Epoch [230/300], Loss: 2.8587\n",
      "Epoch [231/300], Loss: 3.0557\n",
      "Epoch [232/300], Loss: 2.4626\n",
      "Epoch [233/300], Loss: 2.6652\n",
      "Epoch [234/300], Loss: 2.5905\n",
      "Epoch [235/300], Loss: 2.5589\n",
      "Epoch [236/300], Loss: 2.5845\n",
      "Epoch [237/300], Loss: 2.5893\n",
      "Epoch [238/300], Loss: 3.1437\n",
      "Epoch [239/300], Loss: 2.6543\n",
      "Epoch [240/300], Loss: 2.5924\n",
      "Epoch [241/300], Loss: 2.8776\n",
      "Epoch [242/300], Loss: 2.5125\n",
      "Epoch [243/300], Loss: 2.6965\n",
      "Epoch [244/300], Loss: 2.7661\n",
      "Epoch [245/300], Loss: 2.9807\n",
      "Epoch [246/300], Loss: 2.8439\n",
      "Epoch [247/300], Loss: 2.8320\n",
      "Epoch [248/300], Loss: 2.5506\n",
      "Epoch [249/300], Loss: 2.6111\n",
      "Epoch [250/300], Loss: 2.5872\n",
      "Epoch [251/300], Loss: 2.5132\n",
      "Epoch [252/300], Loss: 2.5835\n",
      "Epoch [253/300], Loss: 2.9153\n",
      "Epoch [254/300], Loss: 2.7336\n",
      "Epoch [255/300], Loss: 2.5040\n",
      "Epoch [256/300], Loss: 3.2057\n",
      "Epoch [257/300], Loss: 2.6431\n",
      "Epoch [258/300], Loss: 2.5228\n",
      "Epoch [259/300], Loss: 2.4804\n",
      "Epoch [260/300], Loss: 2.8064\n",
      "Epoch [261/300], Loss: 2.6980\n",
      "Epoch [262/300], Loss: 2.7529\n",
      "Epoch [263/300], Loss: 2.5873\n",
      "Epoch [264/300], Loss: 2.5120\n",
      "Epoch [265/300], Loss: 2.6081\n",
      "Epoch [266/300], Loss: 2.7064\n",
      "Epoch [267/300], Loss: 2.5166\n",
      "Epoch [268/300], Loss: 2.5274\n",
      "Epoch [269/300], Loss: 2.6043\n",
      "Epoch [270/300], Loss: 2.6300\n",
      "Epoch [271/300], Loss: 2.6545\n",
      "Epoch [272/300], Loss: 2.5705\n",
      "Epoch [273/300], Loss: 2.8817\n",
      "Epoch [274/300], Loss: 2.6010\n",
      "Epoch [275/300], Loss: 2.6647\n",
      "Epoch [276/300], Loss: 2.7973\n",
      "Epoch [277/300], Loss: 2.5927\n",
      "Epoch [278/300], Loss: 2.5224\n",
      "Epoch [279/300], Loss: 2.6266\n",
      "Epoch [280/300], Loss: 2.6855\n",
      "Epoch [281/300], Loss: 2.7286\n",
      "Epoch [282/300], Loss: 2.6113\n",
      "Epoch [283/300], Loss: 2.4997\n",
      "Epoch [284/300], Loss: 2.7761\n",
      "Epoch [285/300], Loss: 2.5892\n",
      "Epoch [286/300], Loss: 2.8275\n",
      "Epoch [287/300], Loss: 2.6657\n",
      "Epoch [288/300], Loss: 2.7104\n",
      "Epoch [289/300], Loss: 2.5689\n",
      "Epoch [290/300], Loss: 2.5994\n",
      "Epoch [291/300], Loss: 2.6088\n",
      "Epoch [292/300], Loss: 2.6269\n",
      "Epoch [293/300], Loss: 2.5713\n",
      "Epoch [294/300], Loss: 2.5128\n",
      "Epoch [295/300], Loss: 2.6240\n",
      "Epoch [296/300], Loss: 2.8378\n",
      "Epoch [297/300], Loss: 2.7372\n",
      "Epoch [298/300], Loss: 2.7825\n",
      "Epoch [299/300], Loss: 2.4984\n",
      "Epoch [300/300], Loss: 2.7741\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "train_model(model, train_loader, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 40.3644\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
