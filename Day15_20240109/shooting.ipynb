{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmNWRmMgPZDl",
        "outputId": "29410c33-3841-4d0f-81f7-6ccfb4f71bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ChYhyv_FP9hq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmGInh2QF55",
        "outputId": "4c33e116-fa4a-4492-f90e-8384575d53f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynuo4qodQbd1",
        "outputId": "07caf64e-c030-433a-b88a-8cc0a83146dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r drive/MyDrive/gym_examples /content"
      ],
      "metadata": {
        "id": "47nFL13DQNeP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilSSF7yBmgj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gym_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV2Mt4mZQgfb",
        "outputId": "64e1fcc6-ade1-46a4-cc0b-1803713c54ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "envs  __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode='text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsIXEH5vPeUB",
        "outputId": "b07d87a2-69e5-46e7-af88-72c1f06ac3b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYCQ5y0fP0mO",
        "outputId": "2154601a-288f-4107-a2b3-ec2b4d1ab1fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'tuple'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:167: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space with exception: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space with exception: {e}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utV59F4hmywr",
        "outputId": "74076ef9-9bff-4ae1-9bcf-07a3164a200b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (8, 8, 1), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ2E5wU4QqAj",
        "outputId": "87f832cd-4828-47c4-b263-c692aa19b55f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLJMo4TSQrIy",
        "outputId": "ca8b56cd-352c-4521-9cb7-a609555aaa1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "         | HHHHH   \n",
            "         |   H     \n",
            "         |  HHH    \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,info = env.step((0, 0))"
      ],
      "metadata": {
        "id": "LwsoJLX0nTMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZegK_bTnhr0",
        "outputId": "154861ed-11dd-4ff5-9811-b2a6f61fb1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M        |         \n",
            "         |     HHH \n",
            "         |      H  \n",
            "         |    HHHHH\n",
            "         |      H  \n",
            "         |         \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,info = env.step((4, 4))"
      ],
      "metadata": {
        "id": "rnSgMbtHnppw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah-LJysvn0sq",
        "outputId": "255fd870-4ad0-47fc-9753-182dacddcafe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "         | HHHHH   \n",
            "    M    |   H     \n",
            "         |  HHH    \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,info = env.step((3, 3))\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q2T3wb5n4tL",
        "outputId": "d786e944-c8ef-4d58-f695-d8dc06499a6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "   H     | HHHHH   \n",
            "    M    |   H     \n",
            "         |  HHH    \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward  # Hit일 때의 reward는 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARJXRh1zn-G0",
        "outputId": "978e5dec-4037-4e14-8153-5b9186d04611"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,info = env.step((1, 3))\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj0RBIQCoAPB",
        "outputId": "e6fb3258-29da-48f3-a4b0-981b4fe41c3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "   M     |         \n",
            "         |   H     \n",
            "   H     | HHHHH   \n",
            "    M    |   H     \n",
            "         |  HHH    \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward  # Miss일 때의 reward는 -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrJRnYG6oDpf",
        "outputId": "b70da9bc-afc0-4684-b966-06573dfc1b5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "IK9qxD9t288H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "SSdYfDKwRMOJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_max = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_interval = epsilon_max - epsilon_min\n",
        "batch_size = 16\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 10000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v7gRdlQ3ITb",
        "outputId": "404daa08-9e66-4951-af10-76201281822b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD_N7ySI3iXE",
        "outputId": "29e1a1a8-1566-4327-b023-0fc53df896d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (8, 8, 1), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAs46CeE3p73",
        "outputId": "88d302c9-2742-4bda-eca3-c87650c93d38"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiDiscrete([8 8])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_actions = 64"
      ],
      "metadata": {
        "id": "y3Shy3sL3toi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QModel(nn.Module):\n",
        "  def __init__(self, num_actions):\n",
        "    super(QModel, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(1152, 512)\n",
        "    self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = nn.functional.relu(self.conv1(x))\n",
        "    x = nn.functional.relu(self.conv2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = nn.functional.relu(self.conv3(x))\n",
        "    x = self.flatten(x)\n",
        "    x = nn.functional.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    return self.fc2(x)"
      ],
      "metadata": {
        "id": "FrxC01ff3zHN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = QModel(num_actions)"
      ],
      "metadata": {
        "id": "1IqNrPCX5Ua6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_target = QModel(num_actions)"
      ],
      "metadata": {
        "id": "r25E_DsH5dpJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ],
      "metadata": {
        "id": "vgSKetaK5gYl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []"
      ],
      "metadata": {
        "id": "XH941GPa5xiQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode_reward_history = []\n",
        "running_reward = 0.\n",
        "episode_count = 0\n",
        "frame_count = 0"
      ],
      "metadata": {
        "id": "dXZhFxRY5_VP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 200000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ],
      "metadata": {
        "id": "i1qSeSvE6JDy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_state(obs):\n",
        "  st = torch.from_numpy(obs).squeeze()\n",
        "  st = st.to(torch.int64)\n",
        "  st = torch.nn.functional.one_hot(st, num_classes=3)\n",
        "  st = st.permute(2, 0, 1)\n",
        "  return st.to(torch.float32)"
      ],
      "metadata": {
        "id": "opkmq0qY6bS2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board, info = env.reset()"
      ],
      "metadata": {
        "id": "2u3tsDnh68Fw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3edFrV7A9j",
        "outputId": "ab232a57-7b7c-41cb-8197-9a11ee871f26"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = preprocess_state(board)\n",
        "st.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ASGws87CA0",
        "outputId": "c1b88ef8-98c0-4262-d20d-d70debeb608e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNmJgW_V7HNn",
        "outputId": "a3dbc4f7-e186-43b8-c0e1-4bec6931b007"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_greedy_epsilon(model, state, mask):\n",
        "  global epsilon\n",
        "\n",
        "  #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "  if np.random.rand(1)[0] < epsilon:\n",
        "    action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      # add a batch axis\n",
        "      state_tensor = state.unsqueeze(0)\n",
        "      # compute the q-values\n",
        "      q_values = model(state_tensor)\n",
        "      # select the q-values of valid actions\n",
        "      action = torch.argmax(\n",
        "        q_values.squeeze() + torch.from_numpy(mask) * 100., dim=0)\n",
        "\n",
        "  epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "  epsilon = max(epsilon, epsilon_min)\n",
        "  return action"
      ],
      "metadata": {
        "id": "gbMTCOca7L6J"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "  global epsilon\n",
        "  with torch.no_grad():\n",
        "    state_tensor = state.unsqueeze(0) # batch dimension\n",
        "    q_values = model(state_tensor)\n",
        "    action = torch.argmax(\n",
        "      q_values.squeeze() + torch.from_numpy(mask) * 100.,dim=0)\n",
        "  return action"
      ],
      "metadata": {
        "id": "-M96FnX77z_B"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_batch(_batch_size):\n",
        "  indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "  state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "  state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "  rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "  action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "  # action mask is the mask for the valid actions at the '''next''' state\n",
        "  action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "  done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "  return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ],
      "metadata": {
        "id": "5Ddv42HL7_qM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_network():\n",
        "  state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "    sample_batch(batch_size)\n",
        "\n",
        "  state_sample = torch.tensor(state_sample, dtype=torch.float32)\n",
        "  state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n",
        "  action_sample = torch.tensor(action_sample, dtype=torch.int64)\n",
        "  action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64)\n",
        "  rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n",
        "  done_sample = torch.tensor(done_sample, dtype=torch.float32)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    future_rewards = model_target(state_next_sample)\n",
        "    max_q_values = torch.max (\n",
        "        future_rewards + action_mask_sample * 100.,\n",
        "    dim=1).values.detach() - 100.\n",
        "    target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "  q_values = model(state_sample)\n",
        "  q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "  loss = loss_function(q_values_action, target_q_values)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "JWDqDT-j8xo_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(max_episodes):\n",
        "  state, info = env.reset()\n",
        "  state = preprocess_state(state)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  episode_reward = 0\n",
        "\n",
        "  for timestep in range(1, max_steps_per_episode):\n",
        "    frame_count += 1\n",
        "\n",
        "    action = get_greedy_epsilon(model, state, action_mask)\n",
        "\n",
        "    state_next, reward, done, info = env.step((action // 8, action % 8))\n",
        "    state_next = preprocess_state(state_next)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "    action_history.append(action)\n",
        "    action_mask_history.append(action_mask)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(state_next)\n",
        "    rewards_history.append(reward)\n",
        "    done_history.append(done)\n",
        "\n",
        "    state = state_next\n",
        "\n",
        "    if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "      update_network()\n",
        "\n",
        "    if frame_count % update_target_network == 0:\n",
        "      model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "    if len(rewards_history) > max_memory_length:\n",
        "      del rewards_history[:1]\n",
        "      del state_history[:1]\n",
        "      del state_next_history[:1]\n",
        "      del action_history[:1]\n",
        "      del action_mask_history[:1]\n",
        "      del done_history[:1]\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  episode_count +=1\n",
        "  episode_reward_history.append(episode_reward)\n",
        "\n",
        "  if len(episode_reward_history)> 100:\n",
        "    del episode_reward_history[0]\n",
        "\n",
        "  running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "  if episode_count % 10 == 0:\n",
        "    print(episode_count, frame_count, running_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji7__Lql-IWh",
        "outputId": "6ca3ead1-9a9a-4bc6-f780-3c8993ae93ac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 549 -35.9\n",
            "20 1127 -37.65\n",
            "30 1707 -38.166666666666664\n",
            "40 2265 -37.925\n",
            "50 2838 -38.16\n",
            "60 3411 -38.21666666666667\n",
            "70 3997 -38.55714285714286\n",
            "80 4563 -38.5375\n",
            "90 5145 -38.67777777777778\n",
            "100 5711 -38.57\n",
            "110 6288 -38.91\n",
            "120 6853 -38.78\n",
            "130 7437 -38.88\n",
            "140 8017 -39.1\n",
            "150 8577 -38.87\n",
            "160 9118 -38.53\n",
            "170 9694 -38.33\n",
            "180 10244 -38.09\n",
            "190 10778 -37.57\n",
            "200 11347 -37.64\n",
            "210 11916 -37.46\n",
            "220 12469 -37.26\n",
            "230 13030 -36.95\n",
            "240 13571 -36.48\n",
            "250 14114 -36.35\n",
            "260 14652 -36.28\n",
            "270 15220 -36.2\n",
            "280 15797 -36.51\n",
            "290 16310 -36.24\n",
            "300 16839 -35.76\n",
            "310 17379 -35.55\n",
            "320 17908 -35.27\n",
            "330 18418 -34.68\n",
            "340 18936 -34.41\n",
            "350 19456 -34.14\n",
            "360 19984 -33.98\n",
            "370 20518 -33.56\n",
            "380 21074 -33.33\n",
            "390 21612 -33.58\n",
            "400 22157 -33.72\n",
            "410 22670 -33.33\n",
            "420 23208 -33.46\n",
            "430 23746 -33.84\n",
            "440 24248 -33.68\n",
            "450 24722 -33.14\n",
            "460 25227 -32.93\n",
            "470 25713 -32.47\n",
            "480 26206 -31.74\n",
            "490 26716 -31.46\n",
            "500 27169 -30.54\n",
            "510 27685 -30.59\n",
            "520 28192 -30.22\n",
            "530 28673 -29.53\n",
            "540 29151 -29.27\n",
            "550 29640 -29.42\n",
            "560 30132 -29.29\n",
            "570 30613 -29.22\n",
            "580 31104 -29.18\n",
            "590 31551 -28.49\n",
            "600 32087 -29.36\n",
            "610 32594 -29.31\n",
            "620 33091 -29.19\n",
            "630 33547 -28.94\n",
            "640 34038 -29.07\n",
            "650 34586 -29.68\n",
            "660 35052 -29.46\n",
            "670 35531 -29.44\n",
            "680 36018 -29.42\n",
            "690 36502 -29.79\n",
            "700 36969 -29.04\n",
            "710 37450 -28.72\n",
            "720 37892 -28.17\n",
            "730 38371 -28.4\n",
            "740 38866 -28.44\n",
            "750 39268 -26.96\n",
            "760 39788 -27.5\n",
            "770 40201 -26.82\n",
            "780 40674 -26.68\n",
            "790 41179 -26.91\n",
            "800 41643 -26.86\n",
            "810 42084 -26.44\n",
            "820 42564 -26.86\n",
            "830 42978 -26.21\n",
            "840 43436 -25.84\n",
            "850 43911 -26.59\n",
            "860 44332 -25.54\n",
            "870 44819 -26.28\n",
            "880 45266 -26.0\n",
            "890 45692 -25.19\n",
            "900 46144 -25.07\n",
            "910 46600 -25.22\n",
            "920 47051 -24.89\n",
            "930 47486 -25.1\n",
            "940 47943 -25.09\n",
            "950 48341 -24.3\n",
            "960 48719 -23.87\n",
            "970 49156 -23.39\n",
            "980 49526 -22.62\n",
            "990 49953 -22.63\n",
            "1000 50375 -22.33\n",
            "1010 50821 -22.23\n",
            "1020 51238 -21.89\n",
            "1030 51689 -22.07\n",
            "1040 52145 -22.08\n",
            "1050 52565 -22.3\n",
            "1060 52956 -22.43\n",
            "1070 53373 -22.21\n",
            "1080 53754 -22.32\n",
            "1090 54207 -22.58\n",
            "1100 54572 -22.01\n",
            "1110 54968 -21.51\n",
            "1120 55386 -21.52\n",
            "1130 55800 -21.13\n",
            "1140 56262 -21.19\n",
            "1150 56653 -20.9\n",
            "1160 57013 -20.59\n",
            "1170 57419 -20.48\n",
            "1180 57805 -20.53\n",
            "1190 58196 -19.91\n",
            "1200 58604 -20.34\n",
            "1210 59002 -20.36\n",
            "1220 59406 -20.22\n",
            "1230 59802 -20.04\n",
            "1240 60181 -19.19\n",
            "1250 60590 -19.39\n",
            "1260 60990 -19.81\n",
            "1270 61378 -19.65\n",
            "1280 61816 -20.17\n",
            "1290 62228 -20.42\n",
            "1300 62620 -20.26\n",
            "1310 63073 -20.83\n",
            "1320 63493 -20.99\n",
            "1330 63850 -20.6\n",
            "1340 64221 -20.52\n",
            "1350 64586 -20.06\n",
            "1360 64959 -19.77\n",
            "1370 65378 -20.06\n",
            "1380 65753 -19.43\n",
            "1390 66196 -19.7\n",
            "1400 66574 -19.56\n",
            "1410 66973 -19.0\n",
            "1420 67331 -18.38\n",
            "1430 67728 -18.78\n",
            "1440 68048 -18.27\n",
            "1450 68484 -19.0\n",
            "1460 68810 -18.53\n",
            "1470 69248 -18.72\n",
            "1480 69632 -18.81\n",
            "1490 70000 -18.06\n",
            "1500 70374 -18.02\n",
            "1510 70779 -18.08\n",
            "1520 71190 -18.61\n",
            "1530 71580 -18.54\n",
            "1540 71953 -19.07\n",
            "1550 72357 -18.73\n",
            "1560 72742 -19.32\n",
            "1570 73101 -18.53\n",
            "1580 73484 -18.52\n",
            "1590 73844 -18.44\n",
            "1600 74220 -18.46\n",
            "1610 74559 -17.8\n",
            "1620 74990 -18.02\n",
            "1630 75388 -18.1\n",
            "1640 75714 -17.63\n",
            "1650 76036 -16.81\n",
            "1660 76357 -16.17\n",
            "1670 76674 -15.75\n",
            "1680 77062 -15.8\n",
            "1690 77427 -15.85\n",
            "1700 77797 -15.79\n",
            "1710 78115 -15.58\n",
            "1720 78462 -14.72\n",
            "1730 78815 -14.27\n",
            "1740 79195 -14.81\n",
            "1750 79550 -15.14\n",
            "1760 79851 -14.94\n",
            "1770 80165 -14.91\n",
            "1780 80515 -14.53\n",
            "1790 80869 -14.42\n",
            "1800 81209 -14.12\n",
            "1810 81527 -14.12\n",
            "1820 81901 -14.39\n",
            "1830 82222 -14.07\n",
            "1840 82540 -13.45\n",
            "1850 82910 -13.6\n",
            "1860 83221 -13.7\n",
            "1870 83581 -14.16\n",
            "1880 83882 -13.67\n",
            "1890 84262 -13.93\n",
            "1900 84583 -13.74\n",
            "1910 84848 -13.21\n",
            "1920 85218 -13.17\n",
            "1930 85576 -13.54\n",
            "1940 85892 -13.52\n",
            "1950 86182 -12.72\n",
            "1960 86484 -12.63\n",
            "1970 86787 -12.06\n",
            "1980 87101 -12.19\n",
            "1990 87400 -11.38\n",
            "2000 87700 -11.17\n",
            "2010 88037 -11.89\n",
            "2020 88354 -11.36\n",
            "2030 88662 -10.86\n",
            "2040 88989 -10.97\n",
            "2050 89299 -11.17\n",
            "2060 89605 -11.21\n",
            "2070 89887 -11.0\n",
            "2080 90217 -11.16\n",
            "2090 90521 -11.21\n",
            "2100 90843 -11.43\n",
            "2110 91175 -11.38\n",
            "2120 91473 -11.19\n",
            "2130 91796 -11.34\n",
            "2140 92043 -10.54\n",
            "2150 92325 -10.26\n",
            "2160 92663 -10.58\n",
            "2170 92980 -10.93\n",
            "2180 93300 -10.83\n",
            "2190 93601 -10.8\n",
            "2200 93904 -10.61\n",
            "2210 94233 -10.58\n",
            "2220 94540 -10.67\n",
            "2230 94867 -10.71\n",
            "2240 95171 -11.28\n",
            "2250 95454 -11.29\n",
            "2260 95770 -11.07\n",
            "2270 96036 -10.56\n",
            "2280 96353 -10.53\n",
            "2290 96643 -10.42\n",
            "2300 96954 -10.5\n",
            "2310 97204 -9.71\n",
            "2320 97518 -9.78\n",
            "2330 97796 -9.29\n",
            "2340 98077 -9.06\n",
            "2350 98344 -8.9\n",
            "2360 98648 -8.78\n",
            "2370 98910 -8.74\n",
            "2380 99171 -8.18\n",
            "2390 99470 -8.27\n",
            "2400 99720 -7.66\n",
            "2410 100007 -8.03\n",
            "2420 100306 -7.88\n",
            "2430 100586 -7.9\n",
            "2440 100894 -8.17\n",
            "2450 101194 -8.5\n",
            "2460 101481 -8.33\n",
            "2470 101790 -8.8\n",
            "2480 102082 -9.11\n",
            "2490 102350 -8.8\n",
            "2500 102640 -9.2\n",
            "2510 102923 -9.16\n",
            "2520 103214 -9.08\n",
            "2530 103498 -9.12\n",
            "2540 103776 -8.82\n",
            "2550 104068 -8.74\n",
            "2560 104337 -8.56\n",
            "2570 104580 -7.9\n",
            "2580 104843 -7.61\n",
            "2590 105101 -7.51\n",
            "2600 105363 -7.23\n",
            "2610 105642 -7.19\n",
            "2620 105931 -7.17\n",
            "2630 106205 -7.07\n",
            "2640 106477 -7.01\n",
            "2650 106778 -7.1\n",
            "2660 107059 -7.22\n",
            "2670 107385 -8.05\n",
            "2680 107646 -8.03\n",
            "2690 107936 -8.35\n",
            "2700 108191 -8.28\n",
            "2710 108441 -7.99\n",
            "2720 108747 -8.16\n",
            "2730 109019 -8.14\n",
            "2740 109283 -8.06\n",
            "2750 109569 -7.91\n",
            "2760 109845 -7.86\n",
            "2770 110121 -7.36\n",
            "2780 110386 -7.4\n",
            "2790 110649 -7.13\n",
            "2800 110904 -7.13\n",
            "2810 111179 -7.38\n",
            "2820 111431 -6.84\n",
            "2830 111702 -6.83\n",
            "2840 112006 -7.23\n",
            "2850 112297 -7.28\n",
            "2860 112541 -6.96\n",
            "2870 112809 -6.88\n",
            "2880 113051 -6.65\n",
            "2890 113317 -6.68\n",
            "2900 113591 -6.87\n",
            "2910 113845 -6.66\n",
            "2920 114108 -6.77\n",
            "2930 114359 -6.57\n",
            "2940 114622 -6.16\n",
            "2950 114886 -5.89\n",
            "2960 115135 -5.94\n",
            "2970 115397 -5.88\n",
            "2980 115648 -5.97\n",
            "2990 115890 -5.73\n",
            "3000 116128 -5.37\n",
            "3010 116371 -5.26\n",
            "3020 116627 -5.19\n",
            "3030 116866 -5.07\n",
            "3040 117115 -4.93\n",
            "3050 117356 -4.7\n",
            "3060 117589 -4.54\n",
            "3070 117847 -4.5\n",
            "3080 118111 -4.63\n",
            "3090 118360 -4.7\n",
            "3100 118591 -4.63\n",
            "3110 118865 -4.94\n",
            "3120 119161 -5.34\n",
            "3130 119386 -5.2\n",
            "3140 119635 -5.2\n",
            "3150 119875 -5.19\n",
            "3160 120109 -5.2\n",
            "3170 120374 -5.27\n",
            "3180 120604 -4.93\n",
            "3190 120850 -4.9\n",
            "3200 121116 -5.25\n",
            "3210 121354 -4.89\n",
            "3220 121627 -4.66\n",
            "3230 121882 -4.96\n",
            "3240 122105 -4.7\n",
            "3250 122330 -4.55\n",
            "3260 122551 -4.42\n",
            "3270 122827 -4.53\n",
            "3280 123065 -4.61\n",
            "3290 123295 -4.45\n",
            "3300 123547 -4.31\n",
            "3310 123813 -4.59\n",
            "3320 124064 -4.37\n",
            "3330 124302 -4.2\n",
            "3340 124518 -4.13\n",
            "3350 124760 -4.3\n",
            "3360 125019 -4.68\n",
            "3370 125270 -4.43\n",
            "3380 125533 -4.68\n",
            "3390 125766 -4.71\n",
            "3400 126010 -4.63\n",
            "3410 126236 -4.23\n",
            "3420 126473 -4.09\n",
            "3430 126727 -4.25\n",
            "3440 126971 -4.53\n",
            "3450 127214 -4.54\n",
            "3460 127477 -4.58\n",
            "3470 127719 -4.49\n",
            "3480 127970 -4.37\n",
            "3490 128216 -4.5\n",
            "3500 128451 -4.41\n",
            "3510 128704 -4.68\n",
            "3520 128937 -4.64\n",
            "3530 129171 -4.44\n",
            "3540 129429 -4.58\n",
            "3550 129642 -4.28\n",
            "3560 129915 -4.38\n",
            "3570 130158 -4.39\n",
            "3580 130413 -4.43\n",
            "3590 130652 -4.36\n",
            "3600 130898 -4.47\n",
            "3610 131157 -4.53\n",
            "3620 131395 -4.58\n",
            "3630 131627 -4.56\n",
            "3640 131878 -4.49\n",
            "3650 132117 -4.75\n",
            "3660 132399 -4.84\n",
            "3670 132635 -4.77\n",
            "3680 132877 -4.64\n",
            "3690 133101 -4.49\n",
            "3700 133324 -4.26\n",
            "3710 133556 -3.99\n",
            "3720 133782 -3.87\n",
            "3730 134031 -4.04\n",
            "3740 134255 -3.77\n",
            "3750 134514 -3.97\n",
            "3760 134762 -3.63\n",
            "3770 135009 -3.74\n",
            "3780 135247 -3.7\n",
            "3790 135475 -3.74\n",
            "3800 135695 -3.71\n",
            "3810 135899 -3.43\n",
            "3820 136139 -3.57\n",
            "3830 136377 -3.46\n",
            "3840 136595 -3.4\n",
            "3850 136806 -2.92\n",
            "3860 137033 -2.71\n",
            "3870 137267 -2.58\n",
            "3880 137513 -2.66\n",
            "3890 137752 -2.77\n",
            "3900 137966 -2.71\n",
            "3910 138196 -2.97\n",
            "3920 138424 -2.85\n",
            "3930 138655 -2.78\n",
            "3940 138903 -3.08\n",
            "3950 139114 -3.08\n",
            "3960 139325 -2.92\n",
            "3970 139557 -2.9\n",
            "3980 139783 -2.7\n",
            "3990 140042 -2.9\n",
            "4000 140253 -2.87\n",
            "4010 140474 -2.78\n",
            "4020 140686 -2.62\n",
            "4030 140900 -2.45\n",
            "4040 141112 -2.09\n",
            "4050 141330 -2.16\n",
            "4060 141526 -2.01\n",
            "4070 141779 -2.22\n",
            "4080 142006 -2.23\n",
            "4090 142222 -1.8\n",
            "4100 142434 -1.81\n",
            "4110 142646 -1.72\n",
            "4120 142875 -1.89\n",
            "4130 143109 -2.09\n",
            "4140 143312 -2.0\n",
            "4150 143511 -1.81\n",
            "4160 143736 -2.1\n",
            "4170 143947 -1.68\n",
            "4180 144165 -1.59\n",
            "4190 144365 -1.43\n",
            "4200 144592 -1.58\n",
            "4210 144817 -1.71\n",
            "4220 145021 -1.46\n",
            "4230 145233 -1.24\n",
            "4240 145448 -1.36\n",
            "4250 145669 -1.58\n",
            "4260 145875 -1.39\n",
            "4270 146108 -1.61\n",
            "4280 146300 -1.35\n",
            "4290 146507 -1.42\n",
            "4300 146718 -1.26\n",
            "4310 146926 -1.09\n",
            "4320 147130 -1.09\n",
            "4330 147319 -0.86\n",
            "4340 147530 -0.82\n",
            "4350 147744 -0.75\n",
            "4360 147962 -0.87\n",
            "4370 148154 -0.46\n",
            "4380 148360 -0.6\n",
            "4390 148563 -0.56\n",
            "4400 148769 -0.51\n",
            "4410 148962 -0.36\n",
            "4420 149177 -0.47\n",
            "4430 149389 -0.7\n",
            "4440 149591 -0.61\n",
            "4450 149783 -0.39\n",
            "4460 149975 -0.13\n",
            "4470 150194 -0.4\n",
            "4480 150407 -0.47\n",
            "4490 150588 -0.25\n",
            "4500 150813 -0.44\n",
            "4510 151014 -0.52\n",
            "4520 151192 -0.15\n",
            "4530 151409 -0.2\n",
            "4540 151623 -0.32\n",
            "4550 151839 -0.56\n",
            "4560 152054 -0.79\n",
            "4570 152260 -0.66\n",
            "4580 152456 -0.49\n",
            "4590 152657 -0.69\n",
            "4600 152869 -0.56\n",
            "4610 153069 -0.55\n",
            "4620 153270 -0.78\n",
            "4630 153480 -0.71\n",
            "4640 153696 -0.73\n",
            "4650 153900 -0.61\n",
            "4660 154083 -0.29\n",
            "4670 154281 -0.21\n",
            "4680 154498 -0.42\n",
            "4690 154700 -0.43\n",
            "4700 154939 -0.7\n",
            "4710 155139 -0.7\n",
            "4720 155355 -0.85\n",
            "4730 155558 -0.78\n",
            "4740 155763 -0.67\n",
            "4750 155948 -0.48\n",
            "4760 156130 -0.47\n",
            "4770 156321 -0.4\n",
            "4780 156511 -0.13\n",
            "4790 156716 -0.16\n",
            "4800 156922 0.17\n",
            "4810 157111 0.28\n",
            "4820 157310 0.45\n",
            "4830 157490 0.68\n",
            "4840 157686 0.77\n",
            "4850 157874 0.74\n",
            "4860 158074 0.56\n",
            "4870 158280 0.41\n",
            "4880 158468 0.43\n",
            "4890 158678 0.38\n",
            "4900 158870 0.52\n",
            "4910 159073 0.38\n",
            "4920 159265 0.45\n",
            "4930 159455 0.35\n",
            "4940 159671 0.15\n",
            "4950 159882 -0.08\n",
            "4960 160069 0.05\n",
            "4970 160262 0.18\n",
            "4980 160457 0.11\n",
            "4990 160648 0.3\n",
            "5000 160837 0.33\n",
            "5010 161038 0.35\n",
            "5020 161226 0.39\n",
            "5030 161428 0.27\n",
            "5040 161617 0.54\n",
            "5050 161816 0.66\n",
            "5060 162018 0.51\n",
            "5070 162222 0.4\n",
            "5080 162412 0.45\n",
            "5090 162604 0.44\n",
            "5100 162806 0.31\n",
            "5110 162978 0.6\n",
            "5120 163167 0.59\n",
            "5130 163373 0.55\n",
            "5140 163558 0.59\n",
            "5150 163738 0.78\n",
            "5160 163942 0.76\n",
            "5170 164138 0.84\n",
            "5180 164312 1.0\n",
            "5190 164509 0.95\n",
            "5200 164705 1.01\n",
            "5210 164894 0.84\n",
            "5220 165087 0.8\n",
            "5230 165273 1.0\n",
            "5240 165471 0.87\n",
            "5250 165675 0.63\n",
            "5260 165862 0.8\n",
            "5270 166064 0.74\n",
            "5280 166248 0.64\n",
            "5290 166433 0.76\n",
            "5300 166627 0.78\n",
            "5310 166814 0.8\n",
            "5320 167002 0.85\n",
            "5330 167192 0.81\n",
            "5340 167388 0.83\n",
            "5350 167562 1.13\n",
            "5360 167754 1.08\n",
            "5370 167945 1.19\n",
            "5380 168153 0.95\n",
            "5390 168326 1.07\n",
            "5400 168522 1.05\n",
            "5410 168689 1.25\n",
            "5420 168873 1.29\n",
            "5430 169061 1.31\n",
            "5440 169249 1.39\n",
            "5450 169429 1.33\n",
            "5460 169610 1.44\n",
            "5470 169798 1.47\n",
            "5480 169970 1.83\n",
            "5490 170180 1.46\n",
            "5500 170387 1.35\n",
            "5510 170557 1.32\n",
            "5520 170758 1.15\n",
            "5530 170942 1.19\n",
            "5540 171154 0.95\n",
            "5550 171351 0.78\n",
            "5560 171529 0.81\n",
            "5570 171714 0.84\n",
            "5580 171888 0.82\n",
            "5590 172065 1.15\n",
            "5600 172264 1.23\n",
            "5610 172440 1.17\n",
            "5620 172639 1.19\n",
            "5630 172831 1.11\n",
            "5640 173011 1.43\n",
            "5650 173192 1.59\n",
            "5660 173366 1.63\n",
            "5670 173536 1.78\n",
            "5680 173722 1.66\n",
            "5690 173907 1.58\n",
            "5700 174103 1.61\n",
            "5710 174265 1.75\n",
            "5720 174453 1.86\n",
            "5730 174653 1.78\n",
            "5740 174835 1.76\n",
            "5750 175024 1.68\n",
            "5760 175198 1.68\n",
            "5770 175369 1.67\n",
            "5780 175547 1.75\n",
            "5790 175736 1.71\n",
            "5800 175933 1.7\n",
            "5810 176093 1.72\n",
            "5820 176270 1.83\n",
            "5830 176463 1.9\n",
            "5840 176654 1.81\n",
            "5850 176854 1.7\n",
            "5860 177024 1.74\n",
            "5870 177204 1.65\n",
            "5880 177368 1.79\n",
            "5890 177548 1.88\n",
            "5900 177738 1.95\n",
            "5910 177937 1.56\n",
            "5920 178101 1.69\n",
            "5930 178274 1.89\n",
            "5940 178442 2.12\n",
            "5950 178616 2.38\n",
            "5960 178809 2.15\n",
            "5970 178993 2.11\n",
            "5980 179168 2.0\n",
            "5990 179355 1.93\n",
            "6000 179549 1.89\n",
            "6010 179714 2.23\n",
            "6020 179894 2.07\n",
            "6030 180065 2.09\n",
            "6040 180253 1.89\n",
            "6050 180419 1.97\n",
            "6060 180592 2.17\n",
            "6070 180789 2.04\n",
            "6080 180979 1.89\n",
            "6090 181176 1.79\n",
            "6100 181356 1.93\n",
            "6110 181519 1.95\n",
            "6120 181706 1.88\n",
            "6130 181881 1.84\n",
            "6140 182078 1.75\n",
            "6150 182247 1.72\n",
            "6160 182420 1.72\n",
            "6170 182594 1.95\n",
            "6180 182773 2.06\n",
            "6190 182953 2.23\n",
            "6200 183154 2.02\n",
            "6210 183339 1.8\n",
            "6220 183502 2.04\n",
            "6230 183700 1.81\n",
            "6240 183864 2.14\n",
            "6250 184036 2.11\n",
            "6260 184209 2.11\n",
            "6270 184394 2.0\n",
            "6280 184570 2.03\n",
            "6290 184744 2.09\n",
            "6300 184902 2.52\n",
            "6310 185065 2.74\n",
            "6320 185243 2.59\n",
            "6330 185407 2.93\n",
            "6340 185583 2.81\n",
            "6350 185745 2.91\n",
            "6360 185926 2.83\n",
            "6370 186104 2.9\n",
            "6380 186284 2.86\n",
            "6390 186466 2.78\n",
            "6400 186637 2.65\n",
            "6410 186795 2.7\n",
            "6420 186958 2.85\n",
            "6430 187123 2.84\n",
            "6440 187299 2.84\n",
            "6450 187466 2.79\n",
            "6460 187630 2.96\n",
            "6470 187797 3.07\n",
            "6480 187980 3.04\n",
            "6490 188140 3.26\n",
            "6500 188313 3.24\n",
            "6510 188501 2.94\n",
            "6520 188672 2.86\n",
            "6530 188848 2.75\n",
            "6540 189009 2.9\n",
            "6550 189197 2.69\n",
            "6560 189364 2.66\n",
            "6570 189521 2.76\n",
            "6580 189692 2.88\n",
            "6590 189868 2.72\n",
            "6600 190036 2.77\n",
            "6610 190190 3.11\n",
            "6620 190353 3.19\n",
            "6630 190524 3.24\n",
            "6640 190693 3.16\n",
            "6650 190872 3.25\n",
            "6660 191031 3.33\n",
            "6670 191200 3.21\n",
            "6680 191370 3.22\n",
            "6690 191534 3.34\n",
            "6700 191715 3.21\n",
            "6710 191884 3.06\n",
            "6720 192057 2.96\n",
            "6730 192217 3.07\n",
            "6740 192391 3.02\n",
            "6750 192564 3.08\n",
            "6760 192732 2.99\n",
            "6770 192890 3.1\n",
            "6780 193057 3.13\n",
            "6790 193203 3.31\n",
            "6800 193393 3.22\n",
            "6810 193565 3.19\n",
            "6820 193729 3.28\n",
            "6830 193899 3.18\n",
            "6840 194059 3.32\n",
            "6850 194233 3.31\n",
            "6860 194403 3.29\n",
            "6870 194567 3.23\n",
            "6880 194724 3.33\n",
            "6890 194902 3.01\n",
            "6900 195058 3.35\n",
            "6910 195225 3.4\n",
            "6920 195414 3.15\n",
            "6930 195574 3.25\n",
            "6940 195744 3.15\n",
            "6950 195915 3.18\n",
            "6960 196081 3.22\n",
            "6970 196248 3.19\n",
            "6980 196416 3.08\n",
            "6990 196578 3.24\n",
            "7000 196752 3.06\n",
            "7010 196910 3.15\n",
            "7020 197073 3.41\n",
            "7030 197248 3.26\n",
            "7040 197402 3.42\n",
            "7050 197589 3.26\n",
            "7060 197766 3.15\n",
            "7070 197924 3.24\n",
            "7080 198086 3.3\n",
            "7090 198252 3.26\n",
            "7100 198417 3.35\n",
            "7110 198576 3.34\n",
            "7120 198737 3.36\n",
            "7130 198881 3.67\n",
            "7140 199037 3.65\n",
            "7150 199198 3.91\n",
            "7160 199358 4.08\n",
            "7170 199524 4.0\n",
            "7180 199672 4.14\n",
            "7190 199832 4.2\n",
            "7200 199984 4.33\n",
            "7210 200166 4.1\n",
            "7220 200336 4.01\n",
            "7230 200498 3.83\n",
            "7240 200662 3.75\n",
            "7250 200817 3.81\n",
            "7260 200969 3.89\n",
            "7270 201141 3.83\n",
            "7280 201315 3.57\n",
            "7290 201467 3.65\n",
            "7300 201625 3.59\n",
            "7310 201802 3.64\n",
            "7320 201971 3.65\n",
            "7330 202132 3.66\n",
            "7340 202285 3.77\n",
            "7350 202441 3.76\n",
            "7360 202610 3.59\n",
            "7370 202777 3.64\n",
            "7380 202944 3.71\n",
            "7390 203106 3.61\n",
            "7400 203252 3.73\n",
            "7410 203404 3.98\n",
            "7420 203561 4.1\n",
            "7430 203716 4.16\n",
            "7440 203889 3.96\n",
            "7450 204054 3.87\n",
            "7460 204205 4.05\n",
            "7470 204372 4.05\n",
            "7480 204516 4.28\n",
            "7490 204666 4.4\n",
            "7500 204819 4.33\n",
            "7510 204979 4.25\n",
            "7520 205140 4.21\n",
            "7530 205311 4.05\n",
            "7540 205467 4.22\n",
            "7550 205620 4.34\n",
            "7560 205785 4.2\n",
            "7570 205954 4.18\n",
            "7580 206099 4.17\n",
            "7590 206260 4.06\n",
            "7600 206424 3.95\n",
            "7610 206583 3.96\n",
            "7620 206733 4.07\n",
            "7630 206887 4.24\n",
            "7640 207054 4.13\n",
            "7650 207214 4.06\n",
            "7660 207377 4.08\n",
            "7670 207547 4.07\n",
            "7680 207706 3.93\n",
            "7690 207879 3.81\n",
            "7700 208038 3.86\n",
            "7710 208217 3.66\n",
            "7720 208381 3.52\n",
            "7730 208550 3.37\n",
            "7740 208709 3.45\n",
            "7750 208879 3.35\n",
            "7760 209041 3.36\n",
            "7770 209184 3.63\n",
            "7780 209329 3.77\n",
            "7790 209479 4.0\n",
            "7800 209648 3.9\n",
            "7810 209799 4.18\n",
            "7820 209964 4.17\n",
            "7830 210123 4.27\n",
            "7840 210283 4.26\n",
            "7850 210438 4.41\n",
            "7860 210607 4.34\n",
            "7870 210757 4.27\n",
            "7880 210908 4.21\n",
            "7890 211080 3.99\n",
            "7900 211248 4.0\n",
            "7910 211394 4.05\n",
            "7920 211555 4.09\n",
            "7930 211718 4.05\n",
            "7940 211881 4.02\n",
            "7950 212047 3.91\n",
            "7960 212197 4.1\n",
            "7970 212366 3.91\n",
            "7980 212516 3.92\n",
            "7990 212675 4.05\n",
            "8000 212835 4.13\n",
            "8010 213001 3.93\n",
            "8020 213178 3.77\n",
            "8030 213327 3.91\n",
            "8040 213496 3.85\n",
            "8050 213651 3.96\n",
            "8060 213811 3.86\n",
            "8070 213977 3.89\n",
            "8080 214139 3.77\n",
            "8090 214289 3.86\n",
            "8100 214447 3.88\n",
            "8110 214593 4.08\n",
            "8120 214756 4.22\n",
            "8130 214925 4.02\n",
            "8140 215085 4.11\n",
            "8150 215246 4.05\n",
            "8160 215399 4.12\n",
            "8170 215569 4.08\n",
            "8180 215736 4.03\n",
            "8190 215904 3.85\n",
            "8200 216068 3.79\n",
            "8210 216228 3.65\n",
            "8220 216395 3.61\n",
            "8230 216555 3.7\n",
            "8240 216709 3.76\n",
            "8250 216866 3.8\n",
            "8260 217008 3.91\n",
            "8270 217175 3.94\n",
            "8280 217330 4.06\n",
            "8290 217485 4.19\n",
            "8300 217653 4.15\n",
            "8310 217810 4.18\n",
            "8320 217968 4.27\n",
            "8330 218133 4.22\n",
            "8340 218289 4.2\n",
            "8350 218431 4.35\n",
            "8360 218607 4.01\n",
            "8370 218767 4.08\n",
            "8380 218935 3.95\n",
            "8390 219093 3.92\n",
            "8400 219264 3.89\n",
            "8410 219425 3.85\n",
            "8420 219588 3.8\n",
            "8430 219747 3.86\n",
            "8440 219909 3.8\n",
            "8450 220075 3.56\n",
            "8460 220229 3.78\n",
            "8470 220396 3.71\n",
            "8480 220544 3.91\n",
            "8490 220701 3.92\n",
            "8500 220854 4.1\n",
            "8510 221013 4.12\n",
            "8520 221180 4.08\n",
            "8530 221342 4.05\n",
            "8540 221503 4.06\n",
            "8550 221661 4.14\n",
            "8560 221822 4.07\n",
            "8570 221974 4.22\n",
            "8580 222138 4.06\n",
            "8590 222306 3.95\n",
            "8600 222472 3.82\n",
            "8610 222630 3.83\n",
            "8620 222788 3.92\n",
            "8630 222948 3.94\n",
            "8640 223101 4.02\n",
            "8650 223270 3.91\n",
            "8660 223451 3.71\n",
            "8670 223613 3.61\n",
            "8680 223770 3.68\n",
            "8690 223928 3.78\n",
            "8700 224103 3.69\n",
            "8710 224266 3.64\n",
            "8720 224432 3.56\n",
            "8730 224591 3.57\n",
            "8740 224752 3.49\n",
            "8750 224905 3.65\n",
            "8760 225067 3.84\n",
            "8770 225221 3.92\n",
            "8780 225370 4.0\n",
            "8790 225520 4.08\n",
            "8800 225671 4.32\n",
            "8810 225840 4.26\n",
            "8820 226015 4.17\n",
            "8830 226168 4.23\n",
            "8840 226332 4.2\n",
            "8850 226488 4.17\n",
            "8860 226640 4.27\n",
            "8870 226814 4.07\n",
            "8880 226980 3.9\n",
            "8890 227145 3.75\n",
            "8900 227325 3.46\n",
            "8910 227487 3.53\n",
            "8920 227649 3.66\n",
            "8930 227813 3.55\n",
            "8940 227968 3.64\n",
            "8950 228138 3.5\n",
            "8960 228294 3.46\n",
            "8970 228463 3.51\n",
            "8980 228631 3.49\n",
            "8990 228794 3.51\n",
            "9000 228963 3.62\n",
            "9010 229137 3.5\n",
            "9020 229292 3.57\n",
            "9030 229443 3.7\n",
            "9040 229594 3.74\n",
            "9050 229761 3.77\n",
            "9060 229918 3.76\n",
            "9070 230063 4.0\n",
            "9080 230231 4.0\n",
            "9090 230388 4.06\n",
            "9100 230548 4.15\n",
            "9110 230705 4.32\n",
            "9120 230859 4.33\n",
            "9130 231019 4.24\n",
            "9140 231191 4.03\n",
            "9150 231353 4.08\n",
            "9160 231505 4.13\n",
            "9170 231662 4.01\n",
            "9180 231823 4.08\n",
            "9190 231984 4.04\n",
            "9200 232154 3.94\n",
            "9210 232321 3.84\n",
            "9220 232487 3.72\n",
            "9230 232664 3.55\n",
            "9240 232810 3.81\n",
            "9250 232970 3.83\n",
            "9260 233121 3.84\n",
            "9270 233288 3.74\n",
            "9280 233438 3.85\n",
            "9290 233584 4.0\n",
            "9300 233748 4.06\n",
            "9310 233902 4.19\n",
            "9320 234058 4.29\n",
            "9330 234214 4.5\n",
            "9340 234378 4.32\n",
            "9350 234544 4.26\n",
            "9360 234718 4.03\n",
            "9370 234874 4.14\n",
            "9380 235026 4.12\n",
            "9390 235203 3.81\n",
            "9400 235372 3.76\n",
            "9410 235537 3.65\n",
            "9420 235711 3.47\n",
            "9430 235883 3.31\n",
            "9440 236041 3.37\n",
            "9450 236199 3.45\n",
            "9460 236365 3.53\n",
            "9470 236539 3.35\n",
            "9480 236708 3.18\n",
            "9490 236853 3.5\n",
            "9500 237003 3.69\n",
            "9510 237153 3.84\n",
            "9520 237306 4.05\n",
            "9530 237472 4.11\n",
            "9540 237625 4.16\n",
            "9550 237776 4.23\n",
            "9560 237932 4.33\n",
            "9570 238096 4.43\n",
            "9580 238257 4.51\n",
            "9590 238420 4.33\n",
            "9600 238575 4.28\n",
            "9610 238724 4.29\n",
            "9620 238883 4.23\n",
            "9630 239034 4.38\n",
            "9640 239190 4.35\n",
            "9650 239359 4.17\n",
            "9660 239529 4.03\n",
            "9670 239673 4.23\n",
            "9680 239830 4.27\n",
            "9690 239982 4.38\n",
            "9700 240156 4.19\n",
            "9710 240311 4.13\n",
            "9720 240467 4.16\n",
            "9730 240617 4.17\n",
            "9740 240765 4.25\n",
            "9750 240918 4.41\n",
            "9760 241082 4.47\n",
            "9770 241250 4.23\n",
            "9780 241410 4.2\n",
            "9790 241564 4.18\n",
            "9800 241719 4.37\n",
            "9810 241875 4.36\n",
            "9820 242043 4.24\n",
            "9830 242196 4.21\n",
            "9840 242355 4.1\n",
            "9850 242502 4.16\n",
            "9860 242650 4.32\n",
            "9870 242811 4.39\n",
            "9880 242977 4.33\n",
            "9890 243140 4.24\n",
            "9900 243300 4.19\n",
            "9910 243459 4.16\n",
            "9920 243607 4.36\n",
            "9930 243766 4.3\n",
            "9940 243910 4.45\n",
            "9950 244066 4.36\n",
            "9960 244237 4.13\n",
            "9970 244398 4.13\n",
            "9980 244551 4.26\n",
            "9990 244702 4.38\n",
            "10000 244851 4.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "while not done:\n",
        "  action = get_greedy_action(model, state, action_mask)\n",
        "  print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "  sys.stdout.flush()\n",
        "  time.sleep(1.0)\n",
        "  clear_output(wait=False)\n",
        "  board, reward, done, info = env.step((action // 8, action % 8))\n",
        "  state = preprocess_state(board)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5_QSgHxCdQe",
        "outputId": "52789bd5-6dfa-47ab-c751-ba23609ee838"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "     M   |         \n",
            "  M H    |     H   \n",
            "  M HMH  |     H H \n",
            "   HHHH  |    HHHH \n",
            "    H H  |     H H \n",
            "    H    |     H   \n",
            "\n"
          ]
        }
      ]
    }
  ]
}